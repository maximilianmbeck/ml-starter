import atexit
import multiprocessing as mp
import os
import time
from dataclasses import dataclass
from typing import Optional, TypeVar

import psutil
from torch.optim.optimizer import Optimizer

from ml.core.config import conf_field
from ml.core.state import State
from ml.core.types import Batch
from ml.lr_schedulers.base import SchedulerAdapter
from ml.models.base import BaseModel
from ml.tasks.base import BaseTask
from ml.trainers.base import BaseTrainer, BaseTrainerConfig


@dataclass
class CPUStatsConfig(BaseTrainerConfig):
    ping_interval: int = conf_field(1, help="How often to check stats (in seconds)")


ConfigT = TypeVar("ConfigT", bound=CPUStatsConfig)


@dataclass(frozen=True)
class CPUStats:
    cpu_percent: float
    mem_percent: float
    max_child_cpu_percent: float
    max_child_mem_percent: float
    num_child_procs: int


def worker(config: ConfigT, queue: "mp.Queue[CPUStats]", pid: int) -> None:
    proc = psutil.Process(pid)

    while True:
        child_procs = list(proc.children(recursive=True))
        cpu_stats = CPUStats(
            cpu_percent=proc.cpu_percent(),
            mem_percent=proc.memory_percent(),
            max_child_cpu_percent=max(p.cpu_percent() for p in child_procs),
            max_child_mem_percent=max(p.memory_percent() for p in child_procs),
            num_child_procs=len(child_procs),
        )
        queue.put(cpu_stats)
        time.sleep(config.ping_interval)


class CPUStatsMixin(BaseTrainer[ConfigT]):
    """Defines a trainer mixin for getting CPU statistics."""

    def __init__(self, config: ConfigT) -> None:
        super().__init__(config)

        self._cpu_stats: Optional[CPUStats] = None
        self._cpu_stats_queue: "mp.Queue[CPUStats]" = mp.Queue()

        proc = mp.Process(target=worker, args=(config, self._cpu_stats_queue, os.getpid()), daemon=True)
        proc.start()
        atexit.register(proc.kill)

    def on_step_start(
        self,
        state: State,
        train_batch: Batch,
        task: BaseTask,
        model: BaseModel,
        optim: Optimizer,
        lr_sched: SchedulerAdapter,
    ) -> None:
        super().on_step_start(state, train_batch, task, model, optim, lr_sched)

        while self._cpu_stats_queue is not None and not self._cpu_stats_queue.empty():
            self._cpu_stats = self._cpu_stats_queue.get()
        if self._cpu_stats is not None:
            self.logger.log_scalar("cpu/percent", self._cpu_stats.cpu_percent, namespace="trainer")
            self.logger.log_scalar("cpu/max_child_percent", self._cpu_stats.max_child_cpu_percent, namespace="trainer")
            self.logger.log_scalar("mem/percent", self._cpu_stats.mem_percent, namespace="trainer")
            self.logger.log_scalar("mem/max_child_percent", self._cpu_stats.max_child_mem_percent, namespace="trainer")
            self.logger.log_scalar("child_procs", self._cpu_stats.num_child_procs, namespace="trainer")
